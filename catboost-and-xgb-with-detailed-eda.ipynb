{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Welcome! I am your Kaggle Real Estate Agent in Ames, Iowa.\n\nThis is my second notebook upload, or lets say challenge, to Kaggle Competitions. Hope you enjoy!\n\nBefore we start, I believe that this notebook will help you to see how a regression problem can be handled from scratch. We will cover some common steps and try to make heuristic comments. \n\nIf you feel overwhelmed with python implementations, you can check Python Data Science Handbook by Jake VanderPlas.<br> Here: https://jakevdp.github.io/PythonDataScienceHandbook/\n\nLet's dive into the challenge. We have a **regression problem** for House Prices and our evaluation metric is **Root Mean Square Error (RMSE)** as mentioned in the competition description. My objective is to use tree-based ML algorithms which are **Random Forest(RF), Gradient Boosting, Extreme Gradient Boosting(XGB), Light Gradient Boosting Machine(LGBM), and CatBoost** as baseline models. Then we will tune the parameters of XGB and CatBoost by using Randomized Search. Finally, we will get the final predictions for our houses.\n\nMoreover, since we optimized 2 models, XGB and CatBoost, we will also average the results in the last step.\n\nWe will go through some common steps:<br>\n1) [Preparing libraries and data](#1)<br>\n2) [Explatory data analysis(EDA)](#2)<br>\n3) [Preprocess the data](#3)<br>\n4) [Model selection and hyperparameter search](#4)<br>\n5) [Final prediction and export](#5)<br>\n6) [Additional: averaging models](#6)\n\nI will give brief explanations for each step. If you have any questions, please let me know :)  ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# 1) Preparing Libraries and Data","metadata":{}},{"cell_type":"code","source":"# Common libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom scipy import stats\nfrom scipy.stats import norm, skew \n\n# Preprocess libraries\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom sklearn.preprocessing import LabelBinarizer, MinMaxScaler, StandardScaler, PowerTransformer, FunctionTransformer # for scaling\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, KFold # for hyperparameter search\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error #evaluation metric\n\n# Tree Based Models\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport lightgbm as lgbm\nfrom lightgbm import LGBMRegressor\nimport catboost\nfrom catboost import CatBoostRegressor\n\nplt.style.use('ggplot') \ncolor_pal = sns.color_palette()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:07:50.820922Z","iopub.execute_input":"2023-02-01T12:07:50.822028Z","iopub.status.idle":"2023-02-01T12:07:53.652686Z","shell.execute_reply.started":"2023-02-01T12:07:50.821920Z","shell.execute_reply":"2023-02-01T12:07:53.651549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain Data\ndf = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\ndf.drop_duplicates()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:07:53.655005Z","iopub.execute_input":"2023-02-01T12:07:53.655933Z","iopub.status.idle":"2023-02-01T12:07:53.796407Z","shell.execute_reply.started":"2023-02-01T12:07:53.655885Z","shell.execute_reply":"2023-02-01T12:07:53.795572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Notes on investigating the data \nBefore EDA part, we should learn the variables and derive some facts from the data. First, let's give an brief introduction of the data by investigating the shape, numerical&categorical variables, missing values, distribution of the target, and sparsity. Our aim is **to reduce the number of variables without losing information about the target** by looking at their missing percentage, sparsity level, and effectiveness on the target (for example \"id\" variable may not be related to the target.) \n\nLets review:\n * Describing variables\n * Missing values\n * Distribution of the target variable\n * Zero or near-zero variance variables\n * Numerical and categorical variables\n * Variable descriptions from the competition","metadata":{}},{"cell_type":"code","source":"# Brief Introduction of Data\nprint(\"---------------Train Dataset Shape---------------\")\ndisplay(df.shape)\nprint(\"---------------Describing Numerical Variables---------------\")\ndisplay(df.describe(exclude=\"object\").T)\nprint(\"---------------Describing Categorical Variables---------------\")\ndisplay(df.describe(include=\"object\").T)\nprint(\"---------------Feature Information---------------\")\ndisplay(df.info())\nprint(\"---------------Missing Value Percentage---------------\")\ndisplay(pd.DataFrame(df.isnull().sum()/df.shape[0], columns=[\"null_percent\"]).query('null_percent > 0').round(4).sort_values(\"null_percent\", ascending=False)*100)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:07:57.315910Z","iopub.execute_input":"2023-02-01T12:07:57.316973Z","iopub.status.idle":"2023-02-01T12:07:57.548758Z","shell.execute_reply.started":"2023-02-01T12:07:57.316930Z","shell.execute_reply":"2023-02-01T12:07:57.547420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we have a regression problem, lets have a quick check on the target. It is a good practice to have a normally distributed target for many models and statistical asssumptions. In case of non-normality, we can use transformations such as log transformation. We will also transform the numerical variables in the preprocessing step.","metadata":{}},{"cell_type":"code","source":"# Target variable distribution\n# Since px library is interactive, it does not show the graph until you run.\n# px.histogram(df['SalePrice'],marginal='box')\n\nsns.histplot(data=df['SalePrice'], kde=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:07:59.736927Z","iopub.execute_input":"2023-02-01T12:07:59.737366Z","iopub.status.idle":"2023-02-01T12:08:00.229159Z","shell.execute_reply.started":"2023-02-01T12:07:59.737329Z","shell.execute_reply":"2023-02-01T12:08:00.227712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# QQ-Plot for the target\nfig = plt.figure()\nres = stats.probplot(df['SalePrice'], plot=plt)\nplt.show()\nprint(f\"Skewness of SalePrice is : {skew(df['SalePrice']):.2f} which is right-skewed. Lets use log transformation to handle skewness.\")","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:00.232072Z","iopub.execute_input":"2023-02-01T12:08:00.232760Z","iopub.status.idle":"2023-02-01T12:08:00.461002Z","shell.execute_reply.started":"2023-02-01T12:08:00.232702Z","shell.execute_reply":"2023-02-01T12:08:00.459618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target variable distribution\nprint(f\"Skewness of SalePrice after the transformation is : {skew(np.log1p(df['SalePrice'])):.2f}\")\n# px.histogram(np.log1p(df[\"SalePrice\"]),marginal='box')\n\nsns.histplot(data=np.log1p(df[\"SalePrice\"]), kde=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:01.330868Z","iopub.execute_input":"2023-02-01T12:08:01.331272Z","iopub.status.idle":"2023-02-01T12:08:01.627871Z","shell.execute_reply.started":"2023-02-01T12:08:01.331238Z","shell.execute_reply":"2023-02-01T12:08:01.626772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for all variables, most frequent value percent to check variance level (zero or near-zero variance).\nmost_freq_percent = {}\nfor col in df.columns: #.select_dtypes(exclude=\"object\")\n    most_freq_percent[col] = df[[col]].value_counts().iloc[0]/df[[col]].count().iloc[0]*100\ndf_most_freq = pd.DataFrame.from_dict(most_freq_percent, orient='index',columns=[\"Most_freq_percent\"]).round(2)\nax = df_most_freq.sort_values(\"Most_freq_percent\", ascending=True).tail(50).plot(kind=\"barh\", figsize=(12,16))\nax.axvline(95, color=\"black\", ls=\"--\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:02.564165Z","iopub.execute_input":"2023-02-01T12:08:02.564642Z","iopub.status.idle":"2023-02-01T12:08:03.782321Z","shell.execute_reply.started":"2023-02-01T12:08:02.564604Z","shell.execute_reply":"2023-02-01T12:08:03.781272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# These variables can be listed as near-zero variance and they may be removed depending on the problem.\ndf_most_freq.sort_values(\"Most_freq_percent\", ascending=False).query('Most_freq_percent>94')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:03.783600Z","iopub.execute_input":"2023-02-01T12:08:03.783945Z","iopub.status.idle":"2023-02-01T12:08:03.799222Z","shell.execute_reply.started":"2023-02-01T12:08:03.783913Z","shell.execute_reply":"2023-02-01T12:08:03.798104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we look at the dataset description from the competition page, some variables are required to be processed. Some processes are listed below,\n\n * Find ordinal variables and order them instead of onehotencoding them.\n * Search notes in the description to deal with missing values. For example, some missing values can be filled with a new class like \"N/A\" or \"OTHER\" instead of imputation.\n * To deal with near-zero variance, we can either use feature selection methods like VarianceThreshold or set a cutoff level to merge the classes below the cutoff into a \"Other\" class. Both of them are optional, because it may cause some problems. Moreover, it can cause information loss but improve the computation performance. \n * We can calculate \"Age\" variable instead of using \"Year\" variables.  \n\nThe dataset variables and notes are given in the table below.","metadata":{}},{"cell_type":"markdown","source":"| #  | Variable      | Description                                                                                        | Data Type             | Count | Missing % | Mean     | Min   | Max    | Most frequent % | Note                                                    |\n|----|---------------|----------------------------------------------------------------------------------------------------|-----------------------|-------|-----------|----------|-------|--------|-----------------|---------------------------------------------------------|\n| 1  | Id            | ID                                                                                                 | Numerical             | 1460  | 0%        | 730.5    | 1     | 1460   | 0.1%            | It can be dropped.                                      |\n| 2  | MSSubClass    | The building class                                                                                 | Categorical - Nominal | 1460  | 0%        | 56.89726 | 20    | 190    | 36.7%           | 15 unique counts, it can be reduced by cutoff function. |\n| 3  | LotFrontage   | Linear feet of street connected to property                                                        | Numerical             | 1201  | 18%       | 70.04996 | 21    | 313    | 11.9%           |                                                         |\n| 4  | LotArea       | Lot size in square feet                                                                            | Numerical             | 1460  | 0%        | 10516.83 | 1300  | 215245 | 1.7%            |                                                         |\n| 5  | OverallQual   | Overall material and finish quality                                                                | Numerical             | 1460  | 0%        | 6.099315 | 1     | 10     | 27.2%           | It can be processed like Categorical - Ordinal          |\n| 6  | OverallCond   | Overall condition rating                                                                           | Numerical             | 1460  | 0%        | 5.575342 | 1     | 9      | 56.2%           | It can be processed like Categorical - Ordinal          |\n| 7  | YearBuilt     | Original construction date                                                                         | Numerical             | 1460  | 0%        | 1971.268 | 1872  | 2010   | 4.6%            | Age can be calculated.                                  |\n| 8  | YearRemodAdd  | Remodel date                                                                                       | Numerical             | 1460  | 0%        | 1984.866 | 1950  | 2010   | 12.2%           | Age can be calculated.                                  |\n| 9  | MasVnrArea    | Masonry veneer area in square feet                                                                 | Numerical             | 1452  | 1%        | 103.6853 | 0     | 1600   | 59.3%           |                                                         |\n| 10 | BsmtFinSF1    | Type 1 finished square feet                                                                        | Numerical             | 1460  | 0%        | 443.6397 | 0     | 5644   | 32.0%           |                                                         |\n| 11 | BsmtFinSF2    | Type 2 finished square feet                                                                        | Numerical             | 1460  | 0%        | 46.54932 | 0     | 1474   | 88.6%           |                                                         |\n| 12 | BsmtUnfSF     | Unfinished square feet of basement area                                                            | Numerical             | 1460  | 0%        | 567.2404 | 0     | 2336   | 8.1%            |                                                         |\n| 13 | TotalBsmtSF   | Total square feet of basement area                                                                 | Numerical             | 1460  | 0%        | 1057.429 | 0     | 6110   | 2.5%            |                                                         |\n| 14 | 1stFlrSF      | First Floor square feet                                                                            | Numerical             | 1460  | 0%        | 1162.627 | 334   | 4692   | 1.7%            |                                                         |\n| 15 | 2ndFlrSF      | Second Floor square feet                                                                           | Numerical             | 1460  | 0%        | 346.9925 | 0     | 2065   | 56.8%           |                                                         |\n| 16 | LowQualFinSF  | Low quality finished square feet (all floors)                                                      | Numerical             | 1460  | 0%        | 5.844521 | 0     | 572    | 98.2%           | It can be handled as near zero variance                 |\n| 17 | GrLivArea     | Above grade (ground) living area square feet                                                       | Numerical             | 1460  | 0%        | 1515.464 | 334   | 5642   | 1.5%            |                                                         |\n| 18 | BsmtFullBath  | Basement full bathrooms                                                                            | Numerical             | 1460  | 0%        | 0.425342 | 0     | 3      | 58.6%           | It can be processed like Categorical - Ordinal          |\n| 19 | BsmtHalfBath  | Basement half bathrooms                                                                            | Numerical             | 1460  | 0%        | 0.057534 | 0     | 2      | 94.4%           | It can be handled as near zero variance                 |\n| 20 | FullBath      | Full bathrooms above grade                                                                         | Numerical             | 1460  | 0%        | 1.565068 | 0     | 3      | 52.6%           | It can be processed like Categorical - Ordinal          |\n| 21 | HalfBath      | Half baths above grade                                                                             | Numerical             | 1460  | 0%        | 0.382877 | 0     | 2      | 62.5%           | It can be processed like Categorical - Ordinal          |\n| 22 | BedroomAbvGr  | Bedrooms above grade                                                                               | Numerical             | 1460  | 0%        | 2.866438 | 0     | 8      | 55.1%           | It can be processed like Categorical - Ordinal          |\n| 23 | KitchenAbvGr  | Kitchens above grade                                                                               | Numerical             | 1460  | 0%        | 1.046575 | 0     | 3      | 95.3%           | It can be handled as near zero variance                 |\n| 24 | TotRmsAbvGrd  | Total rooms above grade (does not include bathrooms)                                               | Numerical             | 1460  | 0%        | 6.517808 | 2     | 14     | 27.5%           | It can be processed like Categorical - Ordinal          |\n| 25 | Fireplaces    | Number of fireplaces                                                                               | Numerical             | 1460  | 0%        | 0.613014 | 0     | 3      | 47.3%           | It can be processed like Categorical - Ordinal          |\n| 26 | GarageYrBlt   | Year garage was built                                                                              | Numerical             | 1379  | 6%        | 1978.506 | 1900  | 2010   | 4.7%            | Age can be calculated.                                  |\n| 27 | GarageCars    | Size of garage in car capacity                                                                     | Numerical             | 1460  | 0%        | 1.767123 | 0     | 4      | 56.4%           | It can be processed like Categorical - Ordinal          |\n| 28 | GarageArea    | Size of garage in square feet                                                                      | Numerical             | 1460  | 0%        | 472.9801 | 0     | 1418   | 5.6%            |                                                         |\n| 29 | WoodDeckSF    | Wood deck area in square feet                                                                      | Numerical             | 1460  | 0%        | 94.24452 | 0     | 857    | 52.1%           |                                                         |\n| 30 | OpenPorchSF   | Open porch area in square feet                                                                     | Numerical             | 1460  | 0%        | 46.66027 | 0     | 547    | 44.9%           |                                                         |\n| 31 | EnclosedPorch | Enclosed porch area in square feet                                                                 | Numerical             | 1460  | 0%        | 21.95411 | 0     | 552    | 85.8%           |                                                         |\n| 32 | 3SsnPorch     | Three season porch area in square feet                                                             | Numerical             | 1460  | 0%        | 3.409589 | 0     | 508    | 98.4%           | It can be handled as near zero variance                 |\n| 33 | ScreenPorch   | Screen porch area in square feet                                                                   | Numerical             | 1460  | 0%        | 15.06096 | 0     | 480    | 92.1%           |                                                         |\n| 34 | PoolArea      | Pool area in square feet                                                                           | Numerical             | 1460  | 0%        | 2.758904 | 0     | 738    | 99.5%           | It can be handled as near zero variance                 |\n| 35 | MiscVal       | $Value of miscellaneous feature                                                                    | Numerical             | 1460  | 0%        | 43.48904 | 0     | 15500  | 96.4%           | It can be handled as near zero variance                 |\n| 36 | MoSold        | Month Sold (MM)                                                                                    | Numerical             | 1460  | 0%        | 6.321918 | 1     | 12     | 17.3%           |                                                         |\n| 37 | YrSold        | Year Sold                                                                                          | Numerical             | 1460  | 0%        | 2007.816 | 2006  | 2010   | 23.2%           | Age can be calculated.                                  |\n| 38 | SalePrice     | the property's sale price in dollars. This is the target variable that   you're trying to predict. | Numerical             | 1460  | 0%        | 180921.2 | 34900 | 755000 | 1.4%            |                                                         |","metadata":{}},{"cell_type":"markdown","source":"| #  | Variable      | Description                                                         | Data Type             | Count | Missing % | Unique | Top     | Frequency | Most frequent % | Note                                          |\n|----|---------------|---------------------------------------------------------------------|-----------------------|-------|-----------|--------|---------|-----------|-----------------|-----------------------------------------------|\n| 39 | MSZoning      | The general zoning classification                                   | Categorical - Nominal | 1460  | 0%        | 5      | RL      | 1151      | 78.8%           | It can be reduced by cutoff function.         |\n| 40 | Street        | Type of road access to property                                     | Categorical - Nominal | 1460  | 0%        | 2      | Pave    | 1454      | 99.6%           | It can be handled as near zero variance       |\n| 41 | Alley         | Type of alley access to property                                    | Categorical - Nominal | 91    | 94%       | 2      | Grvl    | 50        | 54.9%           | It can be dropped.                            |\n| 42 | LotShape      | General shape of property                                           | Categorical - Nominal | 1460  | 0%        | 4      | Reg     | 925       | 63.4%           |                                               |\n| 43 | LandContour   | Flatness of the property                                            | Categorical - Nominal | 1460  | 0%        | 4      | Lvl     | 1311      | 89.8%           |                                               |\n| 44 | Utilities     | Type of utilities available                                         | Categorical - Nominal | 1460  | 0%        | 2      | AllPub  | 1459      | 99.9%           | It can be handled as near zero variance       |\n| 45 | LotConfig     | Lot configuration                                                   | Categorical - Nominal | 1460  | 0%        | 5      | Inside  | 1052      | 72.1%           |                                               |\n| 46 | LandSlope     | Slope of property                                                   | Categorical - Nominal | 1460  | 0%        | 3      | Gtl     | 1382      | 94.7%           | It can be handled as near zero variance       |\n| 47 | Neighborhood  | Physical locations within Ames city limits                          | Categorical - Nominal | 1460  | 0%        | 25     | NAmes   | 225       | 15.4%           | It can be reduced by cutoff function.         |\n| 48 | Condition1    | Proximity to various conditions                                     | Categorical - Nominal | 1460  | 0%        | 9      | Norm    | 1260      | 86.3%           | It can be reduced by cutoff function.         |\n| 49 | Condition2    | Proximity to various conditions (if more than one is present)       | Categorical - Nominal | 1460  | 0%        | 8      | Norm    | 1445      | 99.0%           | It can be handled as near zero variance       |\n| 50 | BldgType      | Type of dwelling                                                    | Categorical - Nominal | 1460  | 0%        | 5      | 1Fam    | 1220      | 83.6%           | It can be reduced by cutoff function.         |\n| 51 | HouseStyle    | Style of dwelling                                                   | Categorical - Nominal | 1460  | 0%        | 8      | 1Story  | 726       | 49.7%           | It can be reduced by cutoff function.         |\n| 52 | RoofStyle     | Type of roof                                                        | Categorical - Nominal | 1460  | 0%        | 6      | Gable   | 1141      | 78.2%           | It can be reduced by cutoff function.         |\n| 53 | RoofMatl      | Roof material                                                       | Categorical - Nominal | 1460  | 0%        | 8      | CompShg | 1434      | 98.2%           | It can be dropped.                            |\n| 54 | Exterior1st   | Exterior covering on house                                          | Categorical - Nominal | 1460  | 0%        | 15     | VinylSd | 515       | 35.3%           | It can be reduced by cutoff function.         |\n| 55 | Exterior2nd   | Exterior covering on house (if more than one material)              | Categorical - Nominal | 1460  | 0%        | 16     | VinylSd | 504       | 34.5%           |                                               |\n| 56 | MasVnrType    | Masonry veneer type                                                 | Categorical - Nominal | 1452  | 1%        | 4      | None    | 864       | 59.5%           | High missing values, it can be imputed.       |\n| 57 | ExterQual     | Evaluates the quality of the material on the exterior               | Categorical - Ordinal | 1460  | 0%        | 4      | TA      | 906       | 62.1%           |                                               |\n| 58 | ExterCond     | Evaluates the present condition of the material on the exterior     | Categorical - Ordinal | 1460  | 0%        | 5      | TA      | 1282      | 87.8%           |                                               |\n| 59 | Foundation    | Type of foundation                                                  | Categorical - Nominal | 1460  | 0%        | 6      | PConc   | 647       | 44.3%           | It can be reduced by cutoff function.         |\n| 60 | BsmtQual      | Evaluates the height of the basement                                | Categorical - Ordinal | 1423  | 3%        | 4      | TA      | 649       | 45.6%           |                                               |\n| 61 | BsmtCond      | Evaluates the general condition of the basement                     | Categorical - Ordinal | 1423  | 3%        | 4      | TA      | 1311      | 92.1%           |                                               |\n| 62 | BsmtExposure  | Refers to walkout or garden level walls                             | Categorical - Ordinal | 1422  | 3%        | 4      | No      | 953       | 67.0%           |                                               |\n| 63 | BsmtFinType1  | Rating of basement finished area                                    | Categorical - Ordinal | 1423  | 3%        | 6      | Unf     | 430       | 30.2%           |                                               |\n| 64 | BsmtFinType2  | Rating of basement finished area (if multiple types)                | Categorical - Ordinal | 1422  | 3%        | 6      | Unf     | 1256      | 88.3%           |                                               |\n| 65 | Heating       | Type of heating                                                     | Categorical - Nominal | 1460  | 0%        | 6      | GasA    | 1428      | 97.8%           | It can be handled as near zero variance       |\n| 66 | HeatingQC     | Heating quality and condition                                       | Categorical - Ordinal | 1460  | 0%        | 5      | Ex      | 741       | 50.8%           |                                               |\n| 67 | CentralAir    | Central air conditioning                                            | Categorical - Binary  | 1460  | 0%        | 2      | Y       | 1365      | 93.5%           |                                               |\n| 68 | Electrical    | Electrical system                                                   | Categorical - Nominal | 1459  | 0%        | 5      | SBrkr   | 1334      | 91.4%           | It can be reduced by cutoff function.         |\n| 69 | KitchenQual   | Kitchen quality                                                     | Categorical - Ordinal | 1460  | 0%        | 4      | TA      | 735       | 50.3%           |                                               |\n| 70 | Functional    | Home functionality (Assume typical unless deductions are warranted) | Categorical - Nominal | 1460  | 0%        | 7      | Typ     | 1360      | 93.2%           | It can be reduced by cutoff function.         |\n| 71 | FireplaceQu   | Fireplace quality                                                   | Categorical - Ordinal | 770   | 47%       | 5      | Gd      | 380       | 49.4%           | Create another category  for   missing values |\n| 72 | GarageType    | Garage location                                                     | Categorical - Nominal | 1379  | 6%        | 6      | Attchd  | 870       | 63.1%           | It can be reduced by cutoff function.         |\n| 73 | GarageFinish  | Interior finish of the garage                                       | Categorical - Ordinal | 1379  | 6%        | 3      | Unf     | 605       | 43.9%           |                                               |\n| 74 | GarageQual    | Garage quality                                                      | Categorical - Ordinal | 1379  | 6%        | 5      | TA      | 1311      | 95.1%           | It can be handled as near zero variance       |\n| 75 | GarageCond    | Garage condition                                                    | Categorical - Ordinal | 1379  | 6%        | 5      | TA      | 1326      | 96.2%           | It can be handled as near zero variance       |\n| 76 | PavedDrive    | Paved driveway                                                      | Categorical - Ordinal | 1460  | 0%        | 3      | Y       | 1340      | 91.8%           |                                               |\n| 77 | PoolQC        | Pool quality                                                        | Categorical - Ordinal | 7     | 100%      | 3      | Gd      | 3         | 42.9%           | It can be dropped.                            |\n| 78 | Fence         | Fence quality                                                       | Categorical - Nominal | 281   | 81%       | 4      | MnPrv   | 157       | 55.9%           | Create another category  for   missing values |\n| 79 | MiscFeature   | Miscellaneous feature not covered in other categories               | Categorical - Nominal | 54    | 96%       | 4      | Shed    | 49        | 90.7%           | It can be dropped.                            |\n| 80 | SaleType      | Type of sale                                                        | Categorical - Nominal | 1460  | 0%        | 9      | WD      | 1267      | 86.8%           | It can be reduced by cutoff function.         |\n| 81 | SaleCondition | Condition of sale                                                   | Categorical - Nominal | 1460  | 0%        | 6      | Normal  | 1198      | 82.1%           | It can be reduced by cutoff function.         |","metadata":{}},{"cell_type":"code","source":"# Lets process the data according to what we have learnt from our investigation.\n\ndef preprocess1(df):\n    # Drop some variables = near zero + missing values\n    drop_list = [\"Id\",\"Utilities\",\"Street\",\"PoolArea\",\"Condition2\",\"3SsnPorch\",\"RoofMatl\",\"LowQualFinSF\",\"Heating\",\"MiscVal\",\"GarageCond\",\"KitchenAbvGr\",\"GarageQual\",\"LandSlope\",\"BsmtHalfBath\",\"PoolQC\",\"MiscFeature\",\"Alley\"]\n    df = df.drop(drop_list,axis=1)\n    \n    # Handling Ordinal variables (you can also use ordinal encoding methods.)\n    dict_quality = {\"Po\":1,\"Fa\":2,\"TA\":3,\"Gd\":4,\"Ex\":5}\n    df[\"BsmtExposure\"] = df[\"BsmtExposure\"].fillna(0).replace({\"No\":1,\"Mn\":2,\"Av\":3,\"Gd\":4})\n    df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].fillna(0).replace({\"Unf\":1,\"LwQ\":2,\"Rec\":3,\"BLQ\":4,\"ALQ\":5,\"GLQ\":6})\n    df[\"BsmtFinType2\"] = df[\"BsmtFinType2\"].fillna(0).replace({\"Unf\":1,\"LwQ\":2,\"Rec\":3,\"BLQ\":4,\"ALQ\":5,\"GLQ\":6})\n    df[\"GarageFinish\"] = df[\"GarageFinish\"].fillna(0).replace({\"Unf\":1,\"RFn\":2,\"Fin\":3})\n    df[\"PavedDrive\"] = df[\"PavedDrive\"].replace({\"N\":1,\"P\":2,\"Y\":3})\n    df[\"CentralAir\"] = LabelBinarizer().fit_transform(df[\"CentralAir\"]) #binary transformation    \n    \n    for i in [\"BsmtCond\",\"BsmtQual\",\"FireplaceQu\"]: # some variables contain NA class in the description\n        df[i] = df[i].fillna(0).replace(dict_quality)\n    \n    for i in [\"ExterQual\",\"ExterCond\",\"HeatingQC\",\"KitchenQual\"]:\n        df[i] = df[i].replace(dict_quality)\n\n\n    # Handling Nominal variables\n    df[\"Fence\"] = df[\"Fence\"].fillna(\"N/A\") #N/A is given in the description\n    df[\"GarageType\"] = df[\"GarageType\"].fillna(\"N/A\") #N/A is given in the description\n    df[\"MSSubClass\"] = df[\"MSSubClass\"].apply(lambda x: f'A{x}') #converting into categorical variable\n    \n    \n    # This part is commented out. As mentioned before you can set cutoffs for your categorical variables.\n    # In case of computation performance issues, we can use it.\n    '''\n    def categoricalCutOff(df, var, cutprc=0.06): \n        dict_tmp = {}\n        cut = round(df.shape[0] * cutprc,0)\n        \n        for i in df[var].value_counts().to_frame(name=\"count\").query('count < @cut').index:\n            dict_tmp[i] = \"Other\"\n            df[var] = df[var].replace(dict_tmp)\n        if df[var].value_counts().shape[0]==2:\n            df[var] = LabelBinarizer().fit_transform(df[var].astype(\"str\"))\n            \n\n    list_cutoff = [\"MSSubClass\", \"MSZoning\",\"LotShape\",\"LandContour\",\"LotConfig\",\"Condition1\",\n                   \"BldgType\",\"HouseStyle\",\"RoofStyle\",\"Exterior1st\",\"Exterior2nd\",\"MasVnrType\",\"Foundation\",\n                   \"Electrical\",\"Functional\",\"GarageType\",\"SaleType\",\"SaleCondition\",\"Fence\"]\n\n    for i in list_cutoff:\n        categoricalCutOff(df,i)\n    categoricalCutOff(df,\"Neighborhood\",0.03) # cutoff is changed for the neighborhood\n    '''\n    \n    def yearControl(x,y):\n        if y > x:\n            return np.nan\n        else:\n            return y\n    \n    #YearBuilt, YearRemodAdd, GarageYrBlt can not be higher than YrSold\n    df[\"YearRemodAdd\"]=df.apply(lambda x: yearControl(x.YrSold,x.YearRemodAdd), axis=1)\n    df[\"YearBuilt\"]=df.apply(lambda x: yearControl(x.YrSold,x.YearBuilt), axis=1)\n    df[\"GarageYrBlt\"]=df.apply(lambda x: yearControl(x.YrSold,x.GarageYrBlt), axis=1)\n\n    #age calculation for year columns\n    df[\"BuiltAge\"] = df[\"YrSold\"] - df[\"YearBuilt\"] - (df[\"YearRemodAdd\"] - df[\"YearBuilt\"]) \n    df[\"GarageAge\"] = df[\"YrSold\"] - df[\"GarageYrBlt\"]\n    df = df.drop([\"YearBuilt\",\"YearRemodAdd\",\"GarageYrBlt\"],axis=1)\n            \n    return df\n\ndf_drop = preprocess1(df)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:06.300799Z","iopub.execute_input":"2023-02-01T12:08:06.301284Z","iopub.status.idle":"2023-02-01T12:08:06.475845Z","shell.execute_reply.started":"2023-02-01T12:08:06.301249Z","shell.execute_reply":"2023-02-01T12:08:06.474407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Explatory Data Analysis(EDA)\n\nIn this step, we will discover some trends and facts by visualizing the variables. We will try to answer some questions like which neighborhood has the highest sale prices, whether we have outliers in living area square feet or which variables are more related to the sale prices(target).","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (6,4))\nax = sns.barplot(x = df_drop.isnull().sum().to_frame(name=\"count\").query('count>0').sort_values(\"count\",ascending=False).index, \n                 y = df_drop.isnull().sum().to_frame(name=\"count\").query('count>0').sort_values(\"count\",ascending=False).values.flatten())\nfor bars in ax.containers:\n    ax.bar_label(bars)\nplt.title(\"Missing Value Counts\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:07.346585Z","iopub.execute_input":"2023-02-01T12:08:07.346983Z","iopub.status.idle":"2023-02-01T12:08:07.590983Z","shell.execute_reply.started":"2023-02-01T12:08:07.346950Z","shell.execute_reply":"2023-02-01T12:08:07.590163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization for Categorical Variables\n\nWe will check the distribution and saleprice averages for each categorical variable.\n\n**Important note and question for you:** Ordinal variables are considered as numerical variables to check their correlations among other numerical variables. However, I am not sure whether it is a good practice especially for binary variables like centralair variable. If you have any recommendation or perspective, please let me know in the comment section :) ","metadata":{}},{"cell_type":"code","source":"list_categorical = df_drop.select_dtypes(include=\"object\").columns.to_list()\nprint(f\"Number of Categorical Variables: {len(list_categorical)}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:09.948273Z","iopub.execute_input":"2023-02-01T12:08:09.949353Z","iopub.status.idle":"2023-02-01T12:08:09.958450Z","shell.execute_reply.started":"2023-02-01T12:08:09.949290Z","shell.execute_reply":"2023-02-01T12:08:09.957269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## categorical var\ndef CountViz(df):\n    ncol = 4 # number of cols in the subplot\n    cat_list=list(df.columns)\n\n    plt.figure(figsize = [20, int(np.ceil(len(cat_list)/ncol))*5]) \n\n    for i in range(len(cat_list)):\n        plt.subplot(int(np.ceil(len(cat_list)/ncol)),ncol,i+1)\n        ax=sns.countplot(data=df,x=cat_list[i],saturation=0.5)\n    \n        for p in ax.patches:\n            percentage = '{:.0f}%'.format(100 * p.get_height()/df.shape[0])\n            x = p.get_x() + p.get_width()/2\n            y = p.get_height() *0.5\n            ax.annotate(percentage, (x, y),ha='center', size='large', color='darkred', weight='semibold')\n    \n        plt.xticks(rotation = 45)\n        plt.ylabel(\"\")\n        plt.xlabel(cat_list[i],fontsize=14)\n    plt.tight_layout()\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:10.635382Z","iopub.execute_input":"2023-02-01T12:08:10.635803Z","iopub.status.idle":"2023-02-01T12:08:10.645794Z","shell.execute_reply.started":"2023-02-01T12:08:10.635767Z","shell.execute_reply":"2023-02-01T12:08:10.644242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CountViz(df_drop[list_categorical])","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:11.776466Z","iopub.execute_input":"2023-02-01T12:08:11.776859Z","iopub.status.idle":"2023-02-01T12:08:16.380435Z","shell.execute_reply.started":"2023-02-01T12:08:11.776829Z","shell.execute_reply":"2023-02-01T12:08:16.379502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets visualize the average sale prices for each categorical variables.\nlist_categorical2= list_categorical.copy()\nlist_categorical2.append(\"SalePrice\")","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:16.382063Z","iopub.execute_input":"2023-02-01T12:08:16.382584Z","iopub.status.idle":"2023-02-01T12:08:16.386993Z","shell.execute_reply.started":"2023-02-01T12:08:16.382554Z","shell.execute_reply":"2023-02-01T12:08:16.386202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def MeanSalePriceViz(df,target):\n    ncol = 4 # number of cols in the subplot\n    cat_list=list(df.columns)\n    cat_list.remove(target)\n    \n    plt.figure(figsize = [20, int(np.ceil(len(cat_list)/ncol))*5]) \n\n    for i in range(len(cat_list)):\n\n        plt.subplot(int(np.ceil(len(cat_list)/ncol)),ncol,i+1)\n        ax=sns.barplot(data=df.groupby(cat_list[i]).agg(\"mean\")[target].to_frame().reset_index(),\n                       x=cat_list[i],y=target,saturation=0.5)\n    \n        for p in ax.patches:\n            #percentage = '{:.0f}%'.format(100 * p.get_height()/df.shape[0])\n            x = p.get_x() + p.get_width()/2\n            y = p.get_height() * 0.5\n            ax.annotate(round(p.get_height(),None), (x, y),ha='center', size='large', color='darkred',\n                        weight='semibold', rotation=90)\n    \n        plt.xticks(rotation = 45)\n        plt.ylabel(\"\")\n        plt.xlabel(cat_list[i],fontsize=14)\n    plt.tight_layout()\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:16.388352Z","iopub.execute_input":"2023-02-01T12:08:16.389384Z","iopub.status.idle":"2023-02-01T12:08:16.427005Z","shell.execute_reply.started":"2023-02-01T12:08:16.389342Z","shell.execute_reply":"2023-02-01T12:08:16.425985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MeanSalePriceViz(df_drop[list_categorical2],\"SalePrice\")","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:16.429401Z","iopub.execute_input":"2023-02-01T12:08:16.429855Z","iopub.status.idle":"2023-02-01T12:08:21.273342Z","shell.execute_reply.started":"2023-02-01T12:08:16.429822Z","shell.execute_reply":"2023-02-01T12:08:21.272182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** We can obtain some facts for subcategories by comparing their distribution and sale price averages. Some examples are given below.\n\n * For Sale Condition (the condition of sale), while \"Partial\" subcategory constitutes of 9%, it has the highest sale prices comparing to the other subcategories. Tree-based models will take it into the consideration. \n * For Garage Type, while \"BuiltIn\" subcategory constitutes of 6%, it has the highest sale prices comparing to the other subcategories.\n * For Fence, while \"N/A\" subcategory constitutes of 81%, it has the highest sale prices comparing to the other subcategories. So, having a fence may not affect the sale price.","metadata":{}},{"cell_type":"markdown","source":"### Visualization for Numerical Variables\n\nLets perform some analysis to investigate the numerical variables\n * Univariate Analysis by using Boxplots and Histograms\n * Multivariate Analysis by using Heatmap\n * Bivariate Analysis by using Pairplots","metadata":{}},{"cell_type":"markdown","source":"### Univariate Analysis - EDA","metadata":{}},{"cell_type":"code","source":"# viz for numericals\nlist_numerical = df_drop.select_dtypes(exclude=\"object\").columns.to_list()\nprint(f\"Number of Numerical Variables: {len(list_numerical)}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:21.275437Z","iopub.execute_input":"2023-02-01T12:08:21.275777Z","iopub.status.idle":"2023-02-01T12:08:21.285725Z","shell.execute_reply.started":"2023-02-01T12:08:21.275746Z","shell.execute_reply":"2023-02-01T12:08:21.282864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_hist(df,var):\n    bw=(df[var].max()-df[var].min())/20\n    g = sns.histplot(x = var, data = df, binwidth=bw)\n    g.set(xlabel=\"\",ylabel=\"\")\n    g.set_title('{}'.format(var))\n    \ndef histViz(df,num_list):\n    ncol=4\n    plt.figure(figsize = [20, int(np.ceil(len(num_list)/ncol))*5]) \n    plt.tight_layout()    \n\n    for i in range(len(num_list)):\n        plt.subplot(int(np.ceil(len(num_list)/ncol)),ncol,i+1)\n        plot_hist(df,num_list[i])\n\n    plt.show()\n    \nhistViz(df_drop[list_numerical],list_numerical)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:21.287365Z","iopub.execute_input":"2023-02-01T12:08:21.287708Z","iopub.status.idle":"2023-02-01T12:08:28.741824Z","shell.execute_reply.started":"2023-02-01T12:08:21.287678Z","shell.execute_reply":"2023-02-01T12:08:28.740726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_box(df,var):\n    bw=(df[var].max()-df[var].min())/20\n    g = sns.boxplot(y = var, data = df, medianprops={'color':'c'})\n    g.set(xlabel=\"\",ylabel=\"\")\n    g.set_title('{}'.format(var))\n    \ndef BoxViz(df,num_list):\n    ncol=4\n    plt.figure(figsize = [20, int(np.ceil(len(num_list)/ncol))*5]) \n    plt.tight_layout()    \n\n    for i in range(len(num_list)):\n        plt.subplot(int(np.ceil(len(num_list)/ncol)),ncol,i+1)\n        plot_box(df,num_list[i])\n\n    plt.show()\n    \nBoxViz(df_drop[list_numerical],list_numerical)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:28.745047Z","iopub.execute_input":"2023-02-01T12:08:28.746012Z","iopub.status.idle":"2023-02-01T12:08:33.358246Z","shell.execute_reply.started":"2023-02-01T12:08:28.745974Z","shell.execute_reply":"2023-02-01T12:08:33.357112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** We can obtain some facts for the numerical variables by comparing their distributions. Some examples are given below.\n\n * Most of our variables do not have normal distribution.\n * We have outliers in some variables such as ground living area square feet(GrLivArea). We will remove outliers at the end of this section. \n * \"Functional\" variable seems sparse. We can remove it manually or use a feature elimination method.","metadata":{}},{"cell_type":"markdown","source":"### Skewness Check \n\nSkewness is required to see the importance of transformation which we will implement in \"Preprocess the data\" step. It is just a quick check.","metadata":{}},{"cell_type":"code","source":"# we may use boxcox transformation for variables having above 0.75 skewness(highly skewed)\nskew_dict = {}\nfor i in list_numerical:\n    skew_dict[i] = skew(df_drop[i].dropna())\ndf_skew = pd.DataFrame.from_dict(skew_dict, orient='index', columns=[\"skewness\"]).sort_values(\"skewness\", ascending=False)\nprint(f\"Number of numerical variables: {df_skew.shape[0]} and Number of highly skewed variables: {df_skew.abs().query('skewness > 0.75').shape[0]}\")\ndf_skew.abs().sort_values(\"skewness\", ascending=False).head(5)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:33.359662Z","iopub.execute_input":"2023-02-01T12:08:33.360708Z","iopub.status.idle":"2023-02-01T12:08:33.395191Z","shell.execute_reply.started":"2023-02-01T12:08:33.360669Z","shell.execute_reply":"2023-02-01T12:08:33.394301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multivariate Analysis - EDA","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(df_drop[list_numerical].corr(), cmap=\"viridis\")\nplt.title(\"Heatmap of Data\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:33.396385Z","iopub.execute_input":"2023-02-01T12:08:33.397406Z","iopub.status.idle":"2023-02-01T12:08:34.505027Z","shell.execute_reply.started":"2023-02-01T12:08:33.397365Z","shell.execute_reply":"2023-02-01T12:08:34.503877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SalePrice pearson corr rates\nround(df_drop[list_numerical].corr()['SalePrice'].abs().sort_values(ascending=False),2).to_frame().iloc[1:6,:]","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:34.506270Z","iopub.execute_input":"2023-02-01T12:08:34.506595Z","iopub.status.idle":"2023-02-01T12:08:34.529934Z","shell.execute_reply.started":"2023-02-01T12:08:34.506566Z","shell.execute_reply":"2023-02-01T12:08:34.528597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** Collinearity is an another issue for Machine Learning Algorithms, especially for parametric ones. For example, Garage Age is related to the Overall Condition of the house. We could handle the collinearity by using dimensionality reduction methods such as PCA. We will let the model handle. \n\nWe have also listed the most related variables to the target above and they make sense. For example, overall quality is of course a key factor for the sale price. ","metadata":{}},{"cell_type":"markdown","source":"### Bivariate Analysis - EDA","metadata":{}},{"cell_type":"code","source":"# Since we have many numerical variables, it does not give properly. We can have look on the most correlated variables.\ncorr_top5_list = df_drop[list_numerical].corr()['SalePrice'].abs().sort_values(ascending=False).to_frame().iloc[0:6,:].index\nsns.pairplot(df_drop[corr_top5_list])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:34.531123Z","iopub.execute_input":"2023-02-01T12:08:34.531456Z","iopub.status.idle":"2023-02-01T12:08:41.335944Z","shell.execute_reply.started":"2023-02-01T12:08:34.531427Z","shell.execute_reply":"2023-02-01T12:08:41.334552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_top10_list = df_drop[list_numerical].corr()['SalePrice'].abs().sort_values(ascending=False).to_frame().iloc[[0,6,7,8,9,10],:].index\nsns.pairplot(df_drop[corr_top10_list])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:41.337638Z","iopub.execute_input":"2023-02-01T12:08:41.338000Z","iopub.status.idle":"2023-02-01T12:08:48.388177Z","shell.execute_reply.started":"2023-02-01T12:08:41.337966Z","shell.execute_reply":"2023-02-01T12:08:48.386991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** we can find some outliers and remove them manually.\n * GrLivArea has some outliers above 4000.\n * TotalBsmtSF has a outlier above 6000.\n * 1stFlrSF has a outlier above 4000. (same points with GrLivArea)\n \nMoreover, it seems 1stFlrSF and TotalBsmtSF are correlated. ","metadata":{}},{"cell_type":"code","source":"# Before Preprocessing, lets perform what we have noted about target transformation and outliers so far.\n\ndef preprocess2(df):\n    # target transformation\n    df[\"SalePrice\"] = np.log1p(df[\"SalePrice\"])\n    \n    #outliers\n    df = df.query('GrLivArea < 4000')\n    df = df.query('TotalBsmtSF < 6000')\n    \n    return df\n\nprint(f'Dataframe shape before preprocess2: {df_drop.shape}')\ndf_drop = preprocess2(df_drop)\nprint(f'Dataframe shape after preprocess2: {df_drop.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:48.391955Z","iopub.execute_input":"2023-02-01T12:08:48.392631Z","iopub.status.idle":"2023-02-01T12:08:48.421434Z","shell.execute_reply.started":"2023-02-01T12:08:48.392577Z","shell.execute_reply":"2023-02-01T12:08:48.420291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Preprocess the data\n\nWe had an initial preprocessing at the beginning by considering the competition description. Now lets prepare the data for ML algorithms. In this step, we will create a column transformer and our strategy is here:\n\n * **Missing Values:** The most frequent class(mode) for categorical variables and iterative imputer with Random Forest for numerical variables will be used. Iterative imputer is chosen because instead of filling with a constant value, getting help from a regressor (RF) is preferable(It is also optional). Why did we use Random Forest? because tree-based models are not sensitive for the scaling of the variables. Therefore, if you want to implement iterative imputer with KNN, it is suggested to perform scaling first. <br>\n * **One Hot Encoding:** Since categorical variables can not be directly used in many models, we will use one hot encoding for nominal variables (not ordinal ones!). If you have issues with computation power, you have to reduce the number of variables of your dataset. In that manner, we perform 2 things: 1) translate ordinal variables into numerical ones 2) reduce the number of subcategories/classes by using variance based feature selection or cutoff function. Both of them are optional if you have high computation power, but they also affect the bias in the data. <br>\n * **Scaling:** It is not necessary for tree-based models, however it is a recommended step. Therefore, the Logarithmic transformation is used. (I also tried BoxCox and Power transformation to see the results) <br>\n \n**One more note:** some tree-based models like CatBoost have its own one hot encoding(OHE) functions. So, instead of OHE in this step, you can use functions supported by models.\n ","metadata":{}},{"cell_type":"code","source":"# Train Test Datasets\nX = df_drop.drop(\"SalePrice\", axis=1)\ny = df_drop[\"SalePrice\"]\n\nXtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.20, random_state=1234)\nprint(f'Xtrain shape: {Xtrain.shape}, ytrain shape: {ytrain.shape}')\nprint(f'Xval shape: {Xval.shape}, yval shape: {yval.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:48.423335Z","iopub.execute_input":"2023-02-01T12:08:48.423774Z","iopub.status.idle":"2023-02-01T12:08:48.437969Z","shell.execute_reply.started":"2023-02-01T12:08:48.423722Z","shell.execute_reply":"2023-02-01T12:08:48.436655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting indices for the transformations\ncategorical_var = Xtrain.select_dtypes(include=\"object\").columns.to_list()\ncat_index_list = Xtrain.columns.to_frame(index=False,name=\"variables\").query('variables in @categorical_var').index.to_list()\n\n# Transformers for categorical values\ncat_pipe = Pipeline([('imputer_frequent', SimpleImputer(strategy=\"most_frequent\")),\n                     ('ohe', OneHotEncoder(handle_unknown=\"ignore\", sparse=False))])\n\n# Transformers for numerical values\nnum_pipe = Pipeline([('iterative impute', IterativeImputer(estimator=RandomForestRegressor(n_estimators=200), max_iter=20, tol=0.01)), \n                     ('log_trans', FunctionTransformer(np.log1p, validate=True))\n                     ])\n\nct = ColumnTransformer([('cat_pipe',cat_pipe,cat_index_list)], \n                       remainder=num_pipe)#num_pipe\n\nXtrain_transformed = pd.DataFrame(ct.fit_transform(Xtrain)) \nXval_transformed = pd.DataFrame(ct.transform(Xval))","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:08:48.439780Z","iopub.execute_input":"2023-02-01T12:08:48.440336Z","iopub.status.idle":"2023-02-01T12:10:16.921981Z","shell.execute_reply.started":"2023-02-01T12:08:48.440290Z","shell.execute_reply":"2023-02-01T12:10:16.920836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Xtrain shape: {Xtrain_transformed.shape}, ytrain shape: {ytrain.shape}')\nprint(f'Xval shape: {Xval_transformed.shape}, yval shape: {yval.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:10:16.923389Z","iopub.execute_input":"2023-02-01T12:10:16.923725Z","iopub.status.idle":"2023-02-01T12:10:16.929840Z","shell.execute_reply.started":"2023-02-01T12:10:16.923693Z","shell.execute_reply":"2023-02-01T12:10:16.928807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Variance-based Feature Selection\n\nWe have 1164 X 207 dimension for our training dataset. Such high dimensional data can cause some problems for ML algorithms, so feature selection is needed. ","metadata":{}},{"cell_type":"code","source":"# Since we have some near-zero variance variables, we can eliminate them by using VarianceThreshold \nfrom sklearn.feature_selection import VarianceThreshold #variance based selection\nvt = VarianceThreshold(threshold=0.05)\nXtrain_transformed = vt.fit_transform(Xtrain_transformed)\nXval_transformed = vt.transform(Xval_transformed)\n\nprint(f'Xtrain shape: {Xtrain_transformed.shape}, ytrain shape: {ytrain.shape}')\nprint(f'Xval shape: {Xval_transformed.shape}, yval shape: {yval.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:22:18.287936Z","iopub.execute_input":"2023-02-01T12:22:18.288343Z","iopub.status.idle":"2023-02-01T12:22:18.322547Z","shell.execute_reply.started":"2023-02-01T12:22:18.288304Z","shell.execute_reply":"2023-02-01T12:22:18.321304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4) Model Selection and Hyperparameter Search\n\nAfter the Preprocessing Step, our dataset is ready for modeling! Let's calculate the baseline scores for our models and evaluate the results. The evaluation metric is stated in the competition description as RMSE, but Mean Absolute Error(MAE) and Mean Absolute Percent Error (MAPE) are also used.","metadata":{}},{"cell_type":"code","source":"# For baseline RMSE\ndef evaluationMetrics(clf,Xtrain,Xval,ytrain,yval,refit=True):\n    \n    dict_output= {}\n    \n    if refit:\n        clf.fit(Xtrain,ytrain)\n    ytrainPred=np.expm1(clf.predict(Xtrain))\n    ytestPred=np.expm1(clf.predict(Xval))\n    ytrain = np.expm1(ytrain)\n    yval = np.expm1(yval)\n    \n    #Metrics\n    dict_output[\"reg_name\"]=type(clf).__name__\n    dict_output[\"train_RMSE\"]=mean_squared_error(ytrain,ytrainPred, squared=False)\n    dict_output[\"test_RMSE\"]=mean_squared_error(yval,ytestPred, squared=False)\n    dict_output[\"train_MAE\"]=mean_absolute_error(ytrain,ytrainPred)\n    dict_output[\"test_MAE\"]=mean_absolute_error(yval,ytestPred)\n    dict_output[\"train_MAPE\"]=mean_absolute_percentage_error(ytrain,ytrainPred)\n    dict_output[\"test_MAPE\"]=mean_absolute_percentage_error(yval,ytestPred)\n    \n\n    return dict_output\n\n# For visualization of the metric results\ndef evaluationPlots(df):\n    ncol=2\n    nrow=(df.shape[1]-1)//2\n\n    fig, axs = plt.subplots(nrow,ncol, figsize=(12,12), sharex=True)\n    axs=axs.flatten()\n    ax=0\n    for i in range(1,df.shape[1]):\n        g = sns.barplot(data=df, x=df.iloc[:,0], y=df.iloc[:,i], ax=axs[ax])\n        ax+=1\n        \n        if df.columns[i].find(\"MAPE\") !=-1: # for percentage results\n            for p in g.patches:\n                values = p.get_height()\n                x = p.get_x() + p.get_width()/2\n                y = p.get_height() *0.5\n                g.annotate(f'{values:.2%}', (x, y),ha='center', size='medium', color='darkred', weight='semibold', rotation=90)\n\n        else:\n            for p in g.patches:\n                values = round(p.get_height(),0)\n                x = p.get_x() + p.get_width()/2\n                y = p.get_height() *0.5\n                g.annotate(f'{values:,}', (x, y),ha='center', size='medium', color='darkred', weight='semibold', rotation=90)\n\n    axs[-2].set_xticks(axs[-2].get_xticks(), axs[-2].get_xticklabels(), rotation=90)\n    axs[-1].set_xticks(axs[-1].get_xticks(), axs[-1].get_xticklabels(), rotation=90)\n    \n    # Want to shareY axis for the same metrics.\n    for i in np.array(range(nrow))*2:\n        axs[i].sharey(axs[i+1])\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:22:23.531197Z","iopub.execute_input":"2023-02-01T12:22:23.531935Z","iopub.status.idle":"2023-02-01T12:22:23.550401Z","shell.execute_reply.started":"2023-02-01T12:22:23.531895Z","shell.execute_reply":"2023-02-01T12:22:23.549272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Regressors\nreg_list=[RandomForestRegressor(n_estimators = 200),\n          GradientBoostingRegressor(n_estimators = 200),\n          XGBRegressor(n_estimators = 200),\n          LGBMRegressor(n_estimators = 200), \n          CatBoostRegressor(n_estimators = 200, verbose=False)\n          ]\n\nfor i in range(len(reg_list)):\n    if i == 0:\n        df_eval=pd.DataFrame(evaluationMetrics(reg_list[i],Xtrain_transformed,Xval_transformed,ytrain,yval),index=[0])\n    else:\n        df_eval=df_eval.append(evaluationMetrics(reg_list[i],Xtrain_transformed,Xval_transformed,ytrain,yval), \n                               ignore_index=True)\n\ndisplay(df_eval)\nevaluationPlots(df_eval)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:22:24.585473Z","iopub.execute_input":"2023-02-01T12:22:24.586171Z","iopub.status.idle":"2023-02-01T12:22:33.910032Z","shell.execute_reply.started":"2023-02-01T12:22:24.586125Z","shell.execute_reply":"2023-02-01T12:22:33.908900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** \n * When we compare the train and test results, it is obvious that we have overfitting issues.\n * MAPE test results are around 10% while train ones are around 3-4%.\n * The top performers are CatBoost and XGB and hyperparameter search is needed to improve their performances.","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameter search for Catboost and XGB\n\nBy hyperparameter search, we can optimize our selected models to increase our evaluation metrics. In that manner, documentations of the models should be checked to see the important parameters and notes. \n\n\n**XGB:** https://xgboost.readthedocs.io/en/stable/parameter.html <br>\n**CatBoost:** https://catboost.ai/en/docs/concepts/python-reference_catboostregressor","metadata":{}},{"cell_type":"code","source":"def evaluationResult(name,ytrain,yval,yhat_train,yhat):\n    \n    # log1p reverse transform\n    ytrain = np.expm1(ytrain)\n    yval = np.expm1(yval)\n    yhat_train = np.expm1(yhat_train)\n    yhat = np.expm1(yhat)\n    \n    # results\n    dict_hyper_output = {}\n    dict_hyper_output[\"reg_name\"]=name\n    dict_hyper_output[\"train_RMSE\"]=mean_squared_error(ytrain,yhat_train, squared=False)\n    dict_hyper_output[\"test_RMSE\"]=mean_squared_error(yval,yhat, squared=False)\n    dict_hyper_output[\"train_MAE\"]=mean_absolute_error(ytrain,yhat_train)\n    dict_hyper_output[\"test_MAE\"]=mean_absolute_error(yval,yhat)\n    dict_hyper_output[\"train_MAPE\"]=mean_absolute_percentage_error(ytrain,yhat_train)\n    dict_hyper_output[\"test_MAPE\"]=mean_absolute_percentage_error(yval,yhat)\n    \n    return dict_hyper_output","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:22:44.835076Z","iopub.execute_input":"2023-02-01T12:22:44.835466Z","iopub.status.idle":"2023-02-01T12:22:44.844536Z","shell.execute_reply.started":"2023-02-01T12:22:44.835435Z","shell.execute_reply":"2023-02-01T12:22:44.842755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = KFold(n_splits=5, shuffle=True, random_state=1234)\n\n# Hyperparameter Search for XGB\nXGBParams = {\"n_estimators\" : [100,200,500,1000,1500],\n            \"learning_rate\": np.linspace(0.05,0.3,50), #default=0.3   \n            \"min_child_weight\": range(1,10,2), #default=1\n            \"max_depth\": range(2,10,2),#default=6\n            \"subsample\": [0.2, 0.5, 0.8, 1]}#default=1\n\nrscv_xgb = RandomizedSearchCV(XGBRegressor(objective=\"reg:squarederror\",early_stopping_rounds=25), XGBParams,\n                          cv = cv, return_train_score = True, n_iter=50)\n\n\nrscv_xgb.fit(Xtrain_transformed, ytrain, eval_set=[(Xtrain_transformed, ytrain), (Xval_transformed, yval)], verbose=False)\nyhat_xgb = rscv_xgb.predict(Xval_transformed)\nyhat_train_xgb = rscv_xgb.predict(Xtrain_transformed)\n\nprint(\"Best n_estimators:\", rscv_xgb.best_estimator_.n_estimators)\nprint(\"Best learning_rate:\", rscv_xgb.best_estimator_.learning_rate)\nprint(\"Best min_child_weight:\", rscv_xgb.best_estimator_.min_child_weight)\nprint(\"Best max_depth:\", rscv_xgb.best_estimator_.max_depth)\nprint(\"Best subsample:\", rscv_xgb.best_estimator_.subsample)\n\ndf_eval_hyper=pd.DataFrame(evaluationResult(\"XGBRegressor_hyper\",ytrain,yval,yhat_train_xgb,yhat_xgb),index=[0])\ndisplay(df_eval_hyper)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:22:54.457352Z","iopub.execute_input":"2023-02-01T12:22:54.457773Z","iopub.status.idle":"2023-02-01T12:28:33.630906Z","shell.execute_reply.started":"2023-02-01T12:22:54.457737Z","shell.execute_reply":"2023-02-01T12:28:33.629981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameter Search for CatBoost\nCatParams = {\"iterations\" : [200, 500, 1000, 2000],#default=1000\n            \"learning_rate\": [0.01, 0.015, 0.020, 0.025, 0.03],#default=0.03\n            \"depth\": range(6,11,2), #default=6 \n            \"l2_leaf_reg\": range(3,13,3) #default=3\n            }\n\nrscv_catb = RandomizedSearchCV(CatBoostRegressor(logging_level = \"Silent\", loss_function=\"RMSE\", early_stopping_rounds=25), CatParams,\n                          cv = cv, return_train_score = True, n_iter=50) # you can increase n_iter for better results\n\n\nrscv_catb.fit(Xtrain_transformed, ytrain, eval_set=[(Xtrain_transformed, ytrain), (Xval_transformed, yval)])\nyhat_catb = rscv_catb.predict(Xval_transformed)\nyhat_train_catb = rscv_catb.predict(Xtrain_transformed)\n\nprint(\"Best iterations:\", rscv_catb.best_estimator_.get_param('iterations'))\nprint(\"Best learning_rate:\", rscv_catb.best_estimator_.get_param('learning_rate'))\nprint(\"Best depth:\", rscv_catb.best_estimator_.get_param('depth'))\nprint(\"Best l2_leaf_reg:\", rscv_catb.best_estimator_.get_param('l2_leaf_reg'))\n\n# Metrics\ndf_eval_hyper=df_eval_hyper.append(evaluationResult(\"CatBoostRegressor_hyper\",ytrain,yval,yhat_train_catb,yhat_catb), ignore_index=True)\ndisplay(df_eval_hyper)","metadata":{"execution":{"iopub.status.busy":"2023-02-01T12:30:15.529633Z","iopub.execute_input":"2023-02-01T12:30:15.530050Z","iopub.status.idle":"2023-02-01T13:13:07.051717Z","shell.execute_reply.started":"2023-02-01T12:30:15.530015Z","shell.execute_reply":"2023-02-01T13:13:07.050621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_result = pd.concat([df_eval.query('reg_name in [\"XGBRegressor\", \"CatBoostRegressor\"]'), df_eval_hyper])\nevaluationPlots(eval_result.sort_values(\"reg_name\", ascending=False))","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:13:18.312493Z","iopub.execute_input":"2023-02-01T13:13:18.312897Z","iopub.status.idle":"2023-02-01T13:13:19.457087Z","shell.execute_reply.started":"2023-02-01T13:13:18.312865Z","shell.execute_reply":"2023-02-01T13:13:19.455802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** We have little improvements for both regressor after hyperparameter search. Since CatBoost results are better, we can predict the test data set and submit the results :) \n\nThe results can differ for each run since hyperparameter search is random. If you want you can also save the model after hyperparameter search or set a seed.\n\nAlso, LGBM is a challenger one, you can also optimize it if you want as a practice or shameless self promotion ;)","metadata":{}},{"cell_type":"markdown","source":"# 5) Final Prediction and Export","metadata":{}},{"cell_type":"code","source":"# Preprocess for test \ndf_test2 = preprocess1(df_test) # our initial process step\nXtest_transformed = pd.DataFrame(ct.transform(df_test2)) # our column transformer step\nXtest_transformed = vt.transform(Xtest_transformed) # our variance based feature selection step\n# Test prediction with our model.\nytest_hat = rscv_catb.predict(Xtest_transformed) #rscv_xgb rscv_catb\n# Reverse the logarithmic transformation for the final results\nytest_hat_reverse = np.expm1(ytest_hat)\nytest_hat_reverse","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:13:31.138713Z","iopub.execute_input":"2023-02-01T13:13:31.139129Z","iopub.status.idle":"2023-02-01T13:13:31.550736Z","shell.execute_reply.started":"2023-02-01T13:13:31.139095Z","shell.execute_reply":"2023-02-01T13:13:31.549601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission\ndf_tmp = df_test[[\"Id\"]]\ndf_final = df_tmp.merge(pd.DataFrame(ytest_hat_reverse), left_index=True, right_index=True)\ndf_final=df_final.rename(columns={0:\"SalePrice\"}).set_index(\"Id\").to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:41:35.282510Z","iopub.execute_input":"2023-02-01T13:41:35.282969Z","iopub.status.idle":"2023-02-01T13:41:35.306750Z","shell.execute_reply.started":"2023-02-01T13:41:35.282930Z","shell.execute_reply":"2023-02-01T13:41:35.305436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6) Additional: Averaging Models\n\nSo we have 2 optimized models, why dont we average them to get new results. Stacking models is a good approach and may increase your evaluation metrics. You can also add a new model/learner on the top of our models' results and predict new results. \n\nIf you are interested in stacking models, check here:\nhttps://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard","metadata":{}},{"cell_type":"code","source":"stack_train=np.column_stack([yhat_train_catb,yhat_train_xgb])\nprediction_mean_train = np.mean(stack_train, axis=1)\n\nstack_val = np.column_stack([yhat_catb,yhat_xgb])\nprediction_mean_val = np.mean(stack_val, axis=1)\n\n# Metrics\ndf_eval_hyper=df_eval_hyper.append(evaluationResult(\"StackedModel\",ytrain,yval,prediction_mean_train,prediction_mean_val), ignore_index=True)\ndisplay(df_eval_hyper)\n\neval_result_final = pd.concat([df_eval.query('reg_name in [\"XGBRegressor\", \"CatBoostRegressor\"]'), df_eval_hyper])\nevaluationPlots(eval_result_final.sort_values(\"reg_name\", ascending=False))","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:13:34.962752Z","iopub.execute_input":"2023-02-01T13:13:34.963173Z","iopub.status.idle":"2023-02-01T13:13:36.271195Z","shell.execute_reply.started":"2023-02-01T13:13:34.963136Z","shell.execute_reply":"2023-02-01T13:13:36.270068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess for test \ndf_test2 = preprocess1(df_test)\nXtest_transformed = pd.DataFrame(ct.transform(df_test2))\nXtest_transformed = vt.transform(Xtest_transformed)\n# Test prediction with our model.\nytest_hat_catb = rscv_catb.predict(Xtest_transformed)\nytest_hat_xgb = rscv_xgb.predict(Xtest_transformed)\n\n# get the mean of the models\nstack_test=np.column_stack([ytest_hat_catb,ytest_hat_xgb])\nprediction_mean_test = np.mean(stack_test, axis=1)\n\nytest_hat_mean_reverse = np.expm1(prediction_mean_test)\nytest_hat_mean_reverse","metadata":{"execution":{"iopub.status.busy":"2023-02-01T13:13:53.733157Z","iopub.execute_input":"2023-02-01T13:13:53.733573Z","iopub.status.idle":"2023-02-01T13:13:54.150700Z","shell.execute_reply.started":"2023-02-01T13:13:53.733541Z","shell.execute_reply":"2023-02-01T13:13:54.149622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission\ndf_tmp = df_test[[\"Id\"]]\ndf_final = df_tmp.merge(pd.DataFrame(ytest_hat_mean_reverse), left_index=True, right_index=True)\n#df_final=df_final.rename(columns={0:\"SalePrice\"}).set_index(\"Id\").to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-01-26T09:40:32.375952Z","iopub.execute_input":"2023-01-26T09:40:32.377070Z","iopub.status.idle":"2023-01-26T09:40:32.396736Z","shell.execute_reply.started":"2023-01-26T09:40:32.377016Z","shell.execute_reply":"2023-01-26T09:40:32.395326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you found this notebook helpful, some upvotes would be very much appreciated! :) ","metadata":{}}]}